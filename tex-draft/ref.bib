@article{Zeng2022,
   abstract = {We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4$\times$RTX 3090 (24G) or 8$\times$RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at \url\{https://github.com/THUDM/GLM-130B/\}.},
   author = {Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and Peng Zhang and Yuxiao Dong and Jie Tang},
   month = {10},
   title = {GLM-130B: An Open Bilingual Pre-trained Model},
   url = {http://arxiv.org/abs/2210.02414},
   year = {2022},
}
@article{Zhou2022,
   abstract = {CoqQ is a framework for reasoning about quantum programs in the Coq proof assistant. Its main components are: a deeply embedded quantum programming language, in which classic quantum algorithms are easily expressed, and an expressive program logic for proving properties of programs. CoqQ is foundational: the program logic is formally proved sound with respect to a denotational semantics based on state-of-art mathematical libraries (mathcomp and mathcomp analysis). CoqQ is also practical: assertions can use Dirac expressions, which eases concise specifications, and proofs can exploit local and parallel reasoning, which minimizes verification effort. We illustrate the applicability of CoqQ with many examples from the literature.},
   author = {Li Zhou and Gilles Barthe and Pierre-Yves Strub and Junyi Liu and Mingsheng Ying},
   doi = {10.1145/nnnnnnn.nnnnnnn},
   keywords = {Additional Key Words and Phrases: Quantum Programs,Mathematical Libraries,Program Logics,Proof Assistants},
   month = {7},
   title = {CoqQ: Foundational Verification of Quantum Programs},
   url = {https://arxiv.org/abs/2207.11350v1},
   year = {2022},
}
@article{Arrighi2017,
   abstract = {We provide a computational definition of the notions of vector space and bilinear functions. We use this result to introduce a minimal language combining higher-order computation and linear algebra. This language extends the Lambda-calculus with the possibility to make arbitrary linear combinations of terms alpha.t + beta.u. We describe how to "execute" this language in terms of a few rewrite rules, and justify them through the two fundamental requirements that the language be a language of linear operators, and that it be higher-order. We mention the perspectives of this work in the field of quantum computation, whose circuits we show can be easily encoded in the calculus. Finally, we prove the confluence of the entire calculus.},
   author = {Pablo Arrighi and Gilles Dowek},
   doi = {10.23638/LMCS-13(1:8)2017},
   issn = {1860-5974},
   issue = {1},
   journal = {Logical Methods in Computer Science},
   keywords = {03B40,68N18,81P68,Computer Science,F.1.1,F.4.1,F.4.2,Logic in Computer Science,Programming Languages,Quantum Physics},
   month = {3},
   pages = {1-33},
   publisher = {Episciences.org},
   title = {Lineal: A linear-algebraic Lambda-calculus},
   volume = {Volume 13, Issue 1},
   url = {https://lmcs.episciences.org/3203},
   year = {2017},
}
@article{Biamonte2017,
   abstract = {Tensor network methods are taking a central role in modern quantum physics and beyond. They can provide an efficient approximation to certain classes of quantum states, and the associated graphical language makes it easy to describe and pictorially reason about quantum circuits, channels, protocols, open systems and more. Our goal is to explain tensor networks and some associated methods as quickly and as painlessly as possible. Beginning with the key definitions, the graphical tensor network language is presented through examples. We then provide an introduction to matrix product states. We conclude the tutorial with tensor contractions evaluating combinatorial counting problems. The first one counts the number of solutions for Boolean formulae, whereas the second is Penrose's tensor contraction algorithm, returning the number of $3$-edge-colorings of $3$-regular planar graphs.},
   author = {Jacob Biamonte and Ville Bergholm},
   month = {7},
   title = {Tensor Networks in a Nutshell},
   url = {https://arxiv.org/abs/1708.00006v1},
   year = {2017},
}
@article{,
   abstract = {Language models (LMs) are pretrained to imitate internet text, including content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task performance of standard LM pretraining, both before and after task-specific finetuning. Pretraining with human feedback results in much better preference satisfaction than standard LM pretraining followed by finetuning with feedback, i.e., learning and then unlearning undesirable behavior. Our results suggest that we should move beyond imitation learning when pretraining LMs and incorporate human preferences from the start of training.},
   author = {Tomasz Korbak and Kejian Shi and Angelica Chen and Rasika Bhalerao and Christopher L Buckley and Jason Phang and Samuel R Bowman and Ethan Perez},
   journal = {proceedings.mlr.pressT Korbak, K Shi, A Chen, RV Bhalerao, C Buckley, J Phang, SR Bowman, E PerezInternational Conference on Machine Learning, 2023•proceedings.mlr.press},
   note = {总体而言，结果表明，在预训练LM时，它应该超越模仿学习，并从训练开始就纳入人类的偏好。这是确保语言模型生成符合人类偏好的文本的一大进步，看到这项技术在未来的发展方向令人兴奋！},
   title = {Pretraining language models with human preferences},
   url = {https://proceedings.mlr.press/v202/korbak23a.html},
}
@article{Baader1998,
   abstract = {Intended for students and professionals, this is the first English-language textbook offering a unified and self-contained introduction to the field of term rewriting. 1. Motivating Examples -- 2. Abstract Reduction Systems. 2.1. Equivalence and reduction. 2.2. Well-founded induction. 2.3. Proving termination. 2.4. Lexicographic orders. 2.5. Multiset orders. 2.6. Orders in ML. 2.7. Proving confluence. 2.8. Bibliographic notes -- 3. Universal Algebra. 3.1. Terms, substitutions, and identities. 3.2. Algebras, homomorphisms, and congruences. 3.3. Free algebras. 3.4. Term algebras. 3.5. Equational classes -- 4. Equational Problems. 4.1. Deciding [actual symbol not reproducible] E. 4.2. Term rewriting systems. 4.3. Congruence closure. 4.4. Congruence closure on graphs. 4.5. Syntactic unification. 4.6. Unification by transformation. 4.7. Unification and term rewriting in ML.},
   author = {Franz. Baader and Tobias Nipkow},
   isbn = {0 521 45520 0},
   pages = {301},
   publisher = {Cambridge University Press},
   title = {Term rewriting and all that},
   url = {https://books.google.com/books/about/Term_Rewriting_and_All_That.html?hl=zh-CN&id=N7BvXVUCQk8C},
   year = {1998},
}
@article{Vaswani2017,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
   isbn = {1706.03762v7},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   month = {6},
   pages = {5999-6009},
   publisher = {Neural information processing systems foundation},
   title = {Attention Is All You Need},
   volume = {2017-December},
   url = {https://arxiv.org/abs/1706.03762v7},
   year = {2017},
}
@article{Hoffmann2022,
   abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.},
   author = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Oriol Vinyals and Jack W. Rae and Laurent Sifre},
   isbn = {9781713871088},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   month = {3},
   publisher = {Neural information processing systems foundation},
   title = {Training Compute-Optimal Large Language Models},
   volume = {35},
   url = {https://arxiv.org/abs/2203.15556v1},
   year = {2022},
}
@article{Gu2023,
   abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
   author = {Albert Gu and Tri Dao},
   month = {12},
   title = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
   url = {https://arxiv.org/abs/2312.00752v1},
   year = {2023},
}
@article{Peterson1981,
   abstract = {An extenston of the Knuth-Bendix algorithm for finding complete sets of reductions is described. The extension is intended to handle equational theories which can be split into two parts, R and T, such that each equation m R can be construed as a reduction and T represents an equational theory for which a finite, complete umfication algorithm as known. The extension ts demonstrated in the case when T is the theory of a fimte number of associaUve and commutatwe functions and to which the extension is presently restricted An tmplementatlon of the extended Knuth-Bendtx algorithm has produced complete sets of reductions for free commutative groups, commutative rings wtth umt, and distributive lattices. © 1981, ACM. All rights reserved.},
   author = {Gerald E. Peterson and Mark E. Stickel},
   doi = {10.1145/322248.322251},
   issn = {1557735X},
   issue = {2},
   journal = {Journal of the ACM (JACM)},
   keywords = {Boolean algebra,associative and commutative functions,commutative group,commutative ring,complete sets of reductions,complete unification algorithms,equational theories,finite termination property,lattice,simplification,theorem proving,unique termination property,well-founded partml order},
   month = {4},
   pages = {233-264},
   title = {Complete Sets of Reductions for Some Equational Theories},
   volume = {28},
   year = {1981},
}
@article{Jouannaud1984,
   author = {Jean-Pierre Jouannaud and Helene Kirchner},
   city = {New York, New York, USA},
   doi = {10.1145/800017.800519},
   isbn = {0897911253},
   journal = {Proceedings of the 11th ACM SIGACT-SIGPLAN symposium on Principles of programming languages  - POPL '84},
   pages = {83-92},
   publisher = {ACM Press},
   title = {Completion of a set of rules modulo a set of equations},
   url = {http://portal.acm.org/citation.cfm?doid=800017.800519},
   year = {1984},
}
@article{Yang2023,
   abstract = {Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection: a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's program analysis capability to identify accessible premises and hard negative examples, which makes retrieval much more effective. Furthermore, we construct a new benchmark consisting of 98,734 theorems and proofs extracted from Lean's math library. It features challenging data split requiring the prover to generalize to theorems relying on novel premises that are never used in training. We use this benchmark for training and evaluation, and experimental results demonstrate the effectiveness of ReProver over non-retrieval baselines and GPT-4. We thus provide the first set of open-source LLM-based theorem provers without any proprietary datasets and release it under a permissive MIT license to facilitate further research.},
   author = {Kaiyu Yang and Aidan M Swope and Alex Gu and Rahul Chalamala and Peiyang Song and Shixing Yu and Saad Godil and Ryan Prenger and Anima Anandkumar and Uc Santa Barbara and Ut Austin},
   month = {6},
   title = {LeanDojo: Theorem Proving with Retrieval-Augmented Language Models},
   url = {https://arxiv.org/abs/2306.15626v2},
   year = {2023},
}
@article{Yang2022,
   author = {Kaiyu Yang},
   keywords = {Artificial intelligence,Computer science},
   publisher = {Princeton, NJ : Princeton University},
   title = {Neurosymbolic Machine Learning for Reasoning},
   url = {https://dataspace.princeton.edu/handle/88435/dsp015q47rr958},
   year = {2022},
}
@article{,
   abstract = {We introduce a theorem proving algorithm that uses practically no domain heuris-tics for guiding its connection-style proof search. Instead, it runs many Monte-Carlo simulations guided by reinforcement learning from previous proof attempts. We produce several versions of the prover, parameterized by different learning and guiding algorithms. The strongest version of the system is trained on a large corpus of mathematical problems and evaluated on previously unseen problems. The trained system solves within the same number of inferences over 40% more problems than a baseline prover, which is an unusually high improvement in this hard AI domain. To our knowledge this is the first time reinforcement learning has been convincingly applied to solving general mathematical problems on a large scale.},
   author = {Cezary Kaliszyk and Josef Urban and Henryk Michalewski and Mirek Olšák},
   title = {Reinforcement Learning of Theorem Proving},
}
@article{,
   abstract = {We propose a novel approach to interactive theorem proving (ITP) using deep reinforcement learning. The proposed framework is able to learn proof search strategies as well as tactic and arguments prediction in an end-to-end manner. We formulate the process of ITP as a Markov decision process (MDP) in which each state represents a set of potential derivation paths. This structure allows us to introduce a search mechanism which enables the agent to efficiently discard (predicted) dead-end derivations and restart from promising alternatives. We implement the framework in the HOL4 theorem prover. Experimental results show that the framework using learned search strategies outperforms existing automated theorem provers (i.e. hammers) available in HOL4 when evaluated on unseen problems. We further elaborate the role of key components of the framework using ablation studies.},
   author = {Minchao Wu and Michael Norrish and Christian Walder and Amir Dezfouli},
   title = {TacticZero: Learning to Prove Theorems from Scratch with Deep Reinforcement Learning},
}
@article{,
   abstract = {Labeled data for imitation learning of theorem proving in large libraries of formalized mathematics is scarce as such libraries require years of concentrated effort by human specialists to be built. This is particularly challenging when applying large Transformer language models to tactic prediction, because the scaling of performance with respect to model size is quickly disrupted in the data-scarce, easily-overfitted regime. We propose PACT (Proof Artifact Co-Training), a general methodology for extracting abundant self-supervised data from kernel-level proof terms for co-training alongside the usual tactic prediction objective. We apply this methodology to Lean, an interactive proof assistant which hosts some of the most sophisticated formalized mathematics to date. We instrument Lean with a neural theorem prover driven by a Transformer language model and show that PACT improves theorem proving success rate on a held-out suite of test theorems from 32% to 48%.},
   author = {Jesse Michael Han and Jason Rute and Yuhuai Wu and Edward W Ayers and Stanislas Polu},
   isbn = {2102.06203v1},
   title = {Proof Artifact Co-training for Theorem Proving with Language Models},
}

@article{Arrighi2005,
   abstract = {With a view towards models of quantum computation and/or the interpretation of linear logic, we define a functional language where all functions are linear operators by construction. A small step operational semantic (and hence an interpreter/simulator) is provided for this language in the form of a term rewrite system. The linear-algebraic lambda-calculus hereby constructed is linear in a different (yet related) sense to that, say, of the linear lambda-calculus. These various notions of linearity are discussed in the context of quantum programming languages. KEYWORDS: quantum lambda-calculus, linear lambda-calculus, $\lambda$-calculus, quantum logics.},
   author = {Pablo Arrighi and Gilles Dowek},
   month = {1},
   title = {Linear-algebraic lambda-calculus},
   url = {https://arxiv.org/abs/quant-ph/0501150v1},
   year = {2005},
}
